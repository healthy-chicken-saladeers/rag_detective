{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7FjXss-ziIw",
        "outputId": "1aa21fe4-40f9-4cf9-efff-fc91350a4e2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.12)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.37)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.31.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python finetune_bert.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrL9a-ThztIH",
        "outputId": "088e6c5d-244e-4616-d118-b08bfa9fc262"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-07 06:28:49.208810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Num GPUs Available:  1\n",
            "2023-10-07 06:28:52.253070: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Found GPU at: /device:GPU:0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miankelk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "Encoding is: ISO-8859-1\n",
            "Downloading (â€¦)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 148kB/s]\n",
            "Downloading (â€¦)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 43.3MB/s]\n",
            "Downloading (â€¦)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 35.2MB/s]\n",
            "Downloading (â€¦)lve/main/config.json: 100% 570/570 [00:00<00:00, 3.37MB/s]\n",
            "Downloading model.safetensors: 100% 440M/440M [00:01<00:00, 371MB/s]\n",
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231007_062910-ijpdym3q\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-finetune-run-50agree-10\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/iankelk/bert-sentiment\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/iankelk/bert-sentiment/runs/ijpdym3q\u001b[0m\n",
            "Epoch 1/10\n",
            "424/424 [==============================] - ETA: 0s - loss: 0.5049 - accuracy: 0.7963\n",
            "Calculating validation F1 score...\n",
            "Epoch: 1 - validation_data f1_score: 0.8378\n",
            "424/424 [==============================] - 104s 135ms/step - loss: 0.5049 - accuracy: 0.7963 - val_loss: 0.3968 - val_accuracy: 0.8363\n",
            "Epoch 2/10\n",
            "424/424 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.9183\n",
            "Calculating validation F1 score...\n",
            "Epoch: 2 - validation_data f1_score: 0.8163\n",
            "424/424 [==============================] - 31s 74ms/step - loss: 0.2306 - accuracy: 0.9183 - val_loss: 0.5758 - val_accuracy: 0.8129\n",
            "Epoch 3/10\n",
            "424/424 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9608\n",
            "Calculating validation F1 score...\n",
            "Epoch: 3 - validation_data f1_score: 0.8319\n",
            "424/424 [==============================] - 32s 74ms/step - loss: 0.1259 - accuracy: 0.9608 - val_loss: 0.6491 - val_accuracy: 0.8294\n",
            "Epoch 4/10\n",
            "424/424 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9685\n",
            "Calculating validation F1 score...\n",
            "Epoch: 4 - validation_data f1_score: 0.8446\n",
            "424/424 [==============================] - 32s 75ms/step - loss: 0.0916 - accuracy: 0.9685 - val_loss: 0.5946 - val_accuracy: 0.8446\n",
            "Epoch 5/10\n",
            "424/424 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.9802\n",
            "Calculating validation F1 score...\n",
            "Epoch: 5 - validation_data f1_score: 0.8443\n",
            "424/424 [==============================] - 31s 74ms/step - loss: 0.0641 - accuracy: 0.9802 - val_loss: 0.6158 - val_accuracy: 0.8459\n",
            "Epoch 6/10\n",
            "424/424 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9858\n",
            "Calculating validation F1 score...\n",
            "Epoch: 6 - validation_data f1_score: 0.8430\n",
            "424/424 [==============================] - 31s 74ms/step - loss: 0.0487 - accuracy: 0.9858 - val_loss: 0.6791 - val_accuracy: 0.8418\n",
            "Epoch 7/10\n",
            "424/424 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9906\n",
            "Calculating validation F1 score...\n",
            "Epoch: 7 - validation_data f1_score: 0.8534\n",
            "424/424 [==============================] - 32s 75ms/step - loss: 0.0320 - accuracy: 0.9906 - val_loss: 0.7066 - val_accuracy: 0.8528\n",
            "Epoch 8/10\n",
            "424/424 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9894\n",
            "Calculating validation F1 score...\n",
            "Epoch: 8 - validation_data f1_score: 0.8418\n",
            "424/424 [==============================] - 31s 73ms/step - loss: 0.0373 - accuracy: 0.9894 - val_loss: 0.8494 - val_accuracy: 0.8418\n",
            "Epoch 9/10\n",
            "424/424 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9873\n",
            "Calculating validation F1 score...\n",
            "Epoch: 9 - validation_data f1_score: 0.8366\n",
            "424/424 [==============================] - 31s 74ms/step - loss: 0.0436 - accuracy: 0.9873 - val_loss: 0.6743 - val_accuracy: 0.8363\n",
            "Epoch 10/10\n",
            "424/424 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9932\n",
            "Calculating validation F1 score...\n",
            "Epoch: 10 - validation_data f1_score: 0.8501\n",
            "424/424 [==============================] - 31s 74ms/step - loss: 0.0258 - accuracy: 0.9932 - val_loss: 0.8328 - val_accuracy: 0.8514\n",
            "91/91 [==============================] - 5s 20ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.85      0.80      0.82        91\n",
            "     neutral       0.85      0.91      0.88       432\n",
            "    positive       0.82      0.71      0.76       204\n",
            "\n",
            "    accuracy                           0.84       727\n",
            "   macro avg       0.84      0.81      0.82       727\n",
            "weighted avg       0.84      0.84      0.84       727\n",
            "\n",
            "F1 Score: 0.8392584602932518\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     accuracy â–â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     f1_score â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: val_accuracy â–…â–â–„â–‡â–‡â–†â–ˆâ–†â–…â–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss â–â–„â–…â–„â–„â–…â–†â–ˆâ–…â–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      accuracy 0.99322\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best_epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: best_val_loss 0.39678\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      f1_score 0.83926\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          loss 0.02581\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  val_accuracy 0.85144\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss 0.83284\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mbert-finetune-run-50agree-10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/iankelk/bert-sentiment/runs/ijpdym3q\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231007_062910-ijpdym3q/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2KV1c7az2Yh"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}